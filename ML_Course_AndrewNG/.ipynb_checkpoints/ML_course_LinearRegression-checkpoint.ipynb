{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # needed to plot 3-D surfaces\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data with pandas\n",
    "data = pd.read_csv(os.path.join('Datasets/Data_ex1', 'ex1data1.txt'), sep=\",\", header=None)\n",
    "X,y=data[0],data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(x,y):\n",
    "    plt.figure()\n",
    "    plt.plot(x,y,'ro',ms=10,mec='k')\n",
    "    plt.ylabel('Profit in $10,000',fontsize=14)\n",
    "    plt.xlabel('Population of City in 10,000s',fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "#     plt.show()\n",
    "    \n",
    "plotData(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to implement the following:\n",
    "\n",
    "$Cost function=J(\\theta)=\\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2$\n",
    "\n",
    "where\n",
    "\n",
    "$h_{\\theta}(x)=\\theta^Tx=\\theta_0+\\theta_1x_1$\n",
    "\n",
    "then we need to simultaneously update $\\theta_j$ as:\n",
    "\n",
    "$\\theta_j=\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = y.size  # number of training examples\n",
    "# Introce a column of ones to acommodate $\\theta_0$\n",
    "X = np.stack([np.ones(m), X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement J(\\theta)=1/2m sum_i^m (h(x)-y)^2\n",
    "# where h(x)=theta^T X\n",
    "def computeCost(X, y, theta):\n",
    "    m=0\n",
    "    J=0\n",
    "    \n",
    "    m = y.size  # number of training examples\n",
    "    J= 1/(2*m)*np.sum(np.square(np.dot(X,theta)-y))\n",
    "    \n",
    "    return J\n",
    "\n",
    "# Implement GD\n",
    "def gradientDescent(X,y,theta,alpha,num_iters):\n",
    "    m=y.shape[0]\n",
    "    theta=theta.copy()\n",
    "    J_history=[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        theta_temp=np.zeros_like(theta)\n",
    "        \n",
    "        for j in range(np.shape(theta)[0]):\n",
    "            theta_temp[j]=theta[j] - alpha*(1/m)*np.sum((np.dot(X,theta)-y)*X[:,j])\n",
    "            \n",
    "        theta=theta_temp\n",
    "        \n",
    "        J_history.append(computeCost(X,y,theta))\n",
    "        \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta=np.array([0.0,0.0])\n",
    "J=computeCost(X,y,theta)\n",
    "print(J)\n",
    "\n",
    "theta=np.array([-1,2])\n",
    "J=computeCost(X,y,theta)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize fitting parameters\n",
    "theta = np.zeros(2)\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "\n",
    "theta, J_history = gradientDescent(X ,y, theta, alpha, iterations)\n",
    "print('Theta found by gradient descent: {:.4f}, {:.4f}'.format(*theta))\n",
    "print('Expected theta values (approximately): [-3.6303, 1.1664]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(X[:, 1], y)\n",
    "plt.plot(X[:,1],np.dot(X,theta))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as we already have our prediction model defined we can use it to predict the Y (profit). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict1=np.dot([1,3.5],theta)\n",
    "print('For population = 35,000, we predict a profit of {:.2f}\\n'.format(predict1*10000))\n",
    "\n",
    "predict2=np.dot([1,12.5],theta)\n",
    "print('For population = 12,500, we predict a profit of {:.2f}\\n'.format(predict2*10000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the cost function $J(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a map of J for differente values of theta\n",
    "theta0_vals=np.linspace(-10,10,100)\n",
    "theta1_vals=np.linspace(-1,4,100)\n",
    "\n",
    "# Initialize J_vals (matrix) with 0's\n",
    "J_vals=np.zeros((theta0_vals.shape[0],theta1_vals.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, theta0 in enumerate(theta0_vals):\n",
    "    for j, theta1 in enumerate(theta1_vals):\n",
    "        J_vals[i, j] = computeCost(X, y, [theta0, theta1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of the way meshgrids work in the surf command, we need to\n",
    "# transpose J_vals before calling surf, or else the axes will be flipped\n",
    "J_vals = J_vals.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "# SURFACE PLOT\n",
    "ax = fig.add_subplot(121,projection='3d')\n",
    "ax.plot_surface(theta0_vals,theta1_vals,J_vals,cmap='viridis')\n",
    "plt.xlabel(r'$\\theta_0$',fontsize=16)\n",
    "plt.ylabel(r'$\\theta_1$',fontsize=16)\n",
    "\n",
    "# CONTOUR PLOT\n",
    "ax = plt.subplot(122)\n",
    "plt.contour(theta0_vals,theta1_vals,J_vals,linewidths=2,cmap='viridis',levels=np.logspace(-2,3,20))\n",
    "plt.plot(theta[0],theta[1],'ro',ms=10, lw=2)\n",
    "plt.xlabel(r'$\\theta_0$',fontsize=16)\n",
    "plt.ylabel(r'$\\theta_1$',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with multiple variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data with pandas\n",
    "data = pd.read_csv(os.path.join('Datasets/Data_ex1', 'ex1data2.txt'), sep=\",\", header=None)\n",
    "data=data.astype(float)\n",
    "X = np.stack([data[0].values, data[1].values], axis=1)\n",
    "y = data[2].values\n",
    "m = y.size\n",
    "\n",
    "# data = np.loadtxt(os.path.join('Datasets/Data_ex1', 'ex1data2.txt'), delimiter=',')\n",
    "# X = data[:, :2]\n",
    "# y = data[:, 2]\n",
    "# m = y.size\n",
    "\n",
    "print('{:>8s}{:>8s}{:>10s}'.format('X[:,0]', 'X[:, 1]', 'y'))\n",
    "print('-'*26)\n",
    "for i in range(10):\n",
    "    print('{:8.0f}{:8.0f}{:10.0f}'.format(X[i, 0], X[i, 1], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's scale our variables: feature scale\n",
    "\n",
    "substract the mean and divide by the respective std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(X):\n",
    "    X_norm = X.copy()\n",
    "#     mu = np.zeros(X.shape[1])\n",
    "#     sigma = np.zeros(X.shape[1])\n",
    "\n",
    "    cols = np.shape(X)[-1]\n",
    "    mu = np.zeros(cols)\n",
    "    sigma = np.zeros(cols)\n",
    "    \n",
    "    for i in range(cols):\n",
    "        mu[i] = np.mean(X[:, i])\n",
    "        sigma[i] = np.std(X[:, i])\n",
    "    \n",
    "        X_norm[:, i] = (X[:, i] - mu[i])/sigma[i]\n",
    "    \n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm, mu, sigma = feature_scaling(X)\n",
    "\n",
    "print('Computed mean:', mu)\n",
    "print('Computed standard deviation:', sigma)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{:8.0f}{:8.0f}{:10.0f}'.format(X_norm[i, 0], X_norm[i, 1], y[i]))\n",
    "\n",
    "# add X_0=1\n",
    "X = np.concatenate([np.ones((m,1)),X_norm],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GD multivariate\n",
    "\n",
    "$J(\\theta)=\\frac{1}{2m}(X\\theta-y)^T(X\\theta-y)$\n",
    "\n",
    "$\\theta=\\theta - \\alpha\\frac{1}{m}(X^T(X\\theta-y))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCostMulti(X,y,theta):\n",
    "    m=y.shape[0]\n",
    "    J=0\n",
    "    J = 1/(2*m)*(np.dot((np.dot(X, theta) - y).T, np.dot(X, theta) - y))\n",
    "    \n",
    "    return J\n",
    "\n",
    "def gradientDescentMulti(X,y,theta,alpha,num_iters):\n",
    "    m=y.shape[0]\n",
    "    theta=theta.copy()\n",
    "    J_history=[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        theta = theta - alpha*(1/m)*(np.dot(X.T,np.dot(X,theta)-y))\n",
    "        J_history.append(computeCostMulti(X,y,theta))\n",
    "        \n",
    "    return theta,J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize fitting parameters\n",
    "theta = np.zeros(3)\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 400\n",
    "alpha = 0.1\n",
    "\n",
    "theta, J_history = gradientDescentMulti(X ,y, theta, alpha, iterations)\n",
    "print('theta computed from gradient descent: {:s}'.format(str(theta)))\n",
    "\n",
    "\n",
    "# Plot the convergence graph\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(J_history)), J_history, lw=2)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost J')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict \n",
    "\n",
    "$h_{\\theta}(x)=\\theta^Tx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce the prediction in x_norm\n",
    "# x_norm=[x0,x1,x2] remember x0==1\n",
    "x_norm = [1, (1650-mu[0])/sigma[0], (3-mu[1])/sigma[1]]\n",
    "price = np.dot(np.transpose(theta),x_norm)\n",
    "\n",
    "print('Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): ${:.0f}'.format(price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Equation\n",
    "\n",
    "#### Analytical solution. Works well for small n (approx 1000)\n",
    "\n",
    "$\\theta=(X^TX)^{-1}X^Ty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join('Datasets/Data_ex1', 'ex1data2.txt'), sep=\",\", header=None)\n",
    "data=data.astype(float)\n",
    "X = np.stack([data[0].values, data[1].values], axis=1)\n",
    "y = data[2].values\n",
    "m = y.size\n",
    "X = np.concatenate([np.ones((m, 1)), X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalEq(X,y):\n",
    "    theta=np.zeros(X.shape[1])\n",
    "    \n",
    "    matrix_inv=np.linalg.pinv(np.dot(X.T,X))\n",
    "    theta = np.dot(np.dot(matrix_inv,X.T),y)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = normalEq(X, y);\n",
    "\n",
    "print('Theta computed from the normal equations: {:s}'.format(str(theta)));\n",
    "\n",
    "price = np.dot(np.transpose(theta), [1, 1650, 3])\n",
    "print('Predicted price of a 1650 sq-ft, 3 br house (using normal equations): ${:.0f}'.format(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ldallap/Desktop/GitRep/learning/\")\n",
    "from LDallaP_Functions_Python import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ML_LinearReg_CostFunc_Multi in module LDallaP_Functions_Python:\n",
      "\n",
      "ML_LinearReg_CostFunc_Multi(X, y, theta)\n",
      "    Compute the cost fuction for linear regression with multiple variables\n",
      "    \n",
      "    Equantion implemented\n",
      "    ---------\n",
      "    J(theta) = frac{1}{2m} (X theta - y)^T (X theta -y)\n",
      "    \n",
      "    Parameters\n",
      "    ---------\n",
      "    X: array like. Shape (mxn+1)\n",
      "    y: array like. Shape (m)\n",
      "    theta: array like. Shape (n+1)\n",
      "    \n",
      "    Returns\n",
      "    ---------\n",
      "    J: float. Cost function value\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ML_LinearReg_CostFunc_Multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
